{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Tree Loan Success Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background\n",
    "\n",
    "Economic growth is accelerated by lending money to consumers. It is essential that interest rates are low enough for consumers to make their investments worth it but high enough so banks can continue lending money.  This balancing act becomes essential for a healthy economy. \n",
    "\n",
    "If consumers default on the loan (don’t pay back the full amount of the loan contract) banks cannot predict their revenue and may lead to the banks failing. This is catastrophic to the economy at scale due to less credit available for consumers. The ability to predict loan success is not only vital for bank survival but the robustness of the entire economy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source\n",
    "\n",
    "The dataset used in this analysis is provided by Lending Tree on kaggle.com. The data consists of ~2.3 million loans, ~170 features for each row, ranging from 2003 – 2018. \n",
    "\n",
    "https://www.kaggle.com/wordsforthewise/lending-club\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement + Method\n",
    "\n",
    "The goal of this analysis is to build a model which predicts whether a loan will be successful for Lending Tree which is at least 70% accurate based on historic loans. \n",
    "\n",
    "The dataset contains loans in a variety of in-process stages and complete (fully paid or charged off). For this analysis we are only concerned with fully paid vs charged to feed the model with a known ending. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "\n",
    "The dataset had a large number of features that were duplicates, added no value or NaN values that needed to be updated. This was also where I investigated each feature to get familiar with them. I wanted to maintain as many features as possible in order to maximize the model potential. The model would likely work well with fico score, debt to income ratio, and marital status alone, but decision tree-based models may find hidden patterns that I cannot intuitively understand. \n",
    "\n",
    "Missing Values:\n",
    "All numeric features with missing values were able to be replaced with zero and make sense. Many of these features were number of bankruptcies, number of tax liens, so replacing all with zero if anything added value to each loan. \n",
    "\n",
    "There were certain key features like dti (debt to income ratio) which was so important conceptually that I removed these rows without this information. This is justified because no bank would provide a loan without understanding this ratio.\n",
    "\n",
    "Luckily, the remaining object features with NaN values were minimal (<1%). I replaced these values with ‘Unknown’ to maintain the other feature values for the same row.  \n",
    "\n",
    "Normalizing Strategy\n",
    "It is more important for the model to be good at predicting Charged Off loans vs overall model accuracy. The dataset has roughly 4 fully paid off loans for every charged off loan so I pulled 100,000 loans for each category and proceeded to EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Analysis\n",
    "\n",
    "EDA yielded some inconclusive results. Even on critical features like debt to income ratio and interest rate (based mainly on credit health) there was no clear trend predicting a success loan payoff or not. \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The loan grade calculated by the Lending Tree gives insight into how well they predict loan success based on the same information I have. There is a slight trend to A grades paying off vs F/G grades defaulting, but in both cases some A grades defaulted and some F grades paid off. Interesting!\n",
    "\n",
    " \n",
    "\n",
    "The cleanest trends are shown below. If there was a debt settlement flag, the loan defaulted every time. The loan length (term) also had an interesting trend:  the shorter term lengths was a high indicator on full loan payment. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The state the person lived in also had some interesting trends. The chart above shows which states tended to pay in full vs default. The bottom chart shows which states tended to default vs fully pay. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection\n",
    "\n",
    "Due to the large amount of features and loans in this dataset I decided to go with pycaret to get a quick snapshot on which models performed the best. \n",
    "\n",
    "\n",
    "Right out of the box there were a few models which reached the 70% model accuracy. From this I took the top 2 models and ran pycarets tune_model feature for each. This runs 10 folds to provide a mean for each metric. Both provided similar results.\n",
    "\n",
    "   \n",
    "Chosen model:\n",
    "\n",
    "LightGBM was chosen over catboost due to much less computing power required. This is a gradient boosting framework based on decision trees. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
